{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "softmax.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/cynicaldevil/cs231n-solutions/blob/master/assignment1/softmax.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "kQqzyo-yJqgj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Softmax exercise\n",
        "\n",
        "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
        "\n",
        "This exercise is analogous to the SVM exercise. You will:\n",
        "\n",
        "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
        "- implement the fully-vectorized expression for its **analytic gradient**\n",
        "- **check your implementation** with numerical gradient\n",
        "- use a validation set to **tune the learning rate and regularization** strength\n",
        "- **optimize** the loss function with **SGD**\n",
        "- **visualize** the final learned weights\n"
      ]
    },
    {
      "metadata": {
        "id": "-0njHvKxKGKP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### To cross check if notebook is running on GPU backend"
      ]
    },
    {
      "metadata": {
        "id": "p68ra1TrKOYm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62723106-3a56-4c32-cdd3-e3d32eff0e33"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "53XjkRggKRXN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download and extract the required files"
      ]
    },
    {
      "metadata": {
        "id": "M-4D-PirKVmU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        },
        "outputId": "a1f7546e-40b8-45bf-f72e-7a6f8baae7ce"
      },
      "cell_type": "code",
      "source": [
        "!rm -rf assignment1\n",
        "!wget http://cs231n.github.io/assignments/2018/spring1718_assignment1.zip\n",
        "!unzip spring1718_assignment1.zip\n",
        "!rm spring1718_assignment1.zip\n",
        "!cd ./assignment1 && ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-06-28 06:32:30--  http://cs231n.github.io/assignments/2018/spring1718_assignment1.zip\n",
            "Resolving cs231n.github.io (cs231n.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to cs231n.github.io (cs231n.github.io)|185.199.108.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73256 (72K) [application/zip]\n",
            "Saving to: ‘spring1718_assignment1.zip’\n",
            "\n",
            "spring1718_assignme 100%[===================>]  71.54K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2018-06-28 06:32:31 (7.39 MB/s) - ‘spring1718_assignment1.zip’ saved [73256/73256]\n",
            "\n",
            "Archive:  spring1718_assignment1.zip\n",
            "   creating: assignment1/\n",
            " extracting: assignment1/.gitignore  \n",
            "   creating: assignment1/.ipynb_checkpoints/\n",
            "  inflating: assignment1/.ipynb_checkpoints/features-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/knn-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/softmax-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/svm-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/two_layer_net-checkpoint.ipynb  \n",
            "  inflating: assignment1/collectSubmission.sh  \n",
            "   creating: assignment1/cs231n/\n",
            "   creating: assignment1/cs231n/classifiers/\n",
            "  inflating: assignment1/cs231n/classifiers/k_nearest_neighbor.py  \n",
            "  inflating: assignment1/cs231n/classifiers/linear_classifier.py  \n",
            "  inflating: assignment1/cs231n/classifiers/linear_svm.py  \n",
            "  inflating: assignment1/cs231n/classifiers/neural_net.py  \n",
            "  inflating: assignment1/cs231n/classifiers/softmax.py  \n",
            "  inflating: assignment1/cs231n/classifiers/__init__.py  \n",
            "   creating: assignment1/cs231n/datasets/\n",
            "  inflating: assignment1/cs231n/datasets/.gitignore  \n",
            "  inflating: assignment1/cs231n/datasets/get_datasets.sh  \n",
            "  inflating: assignment1/cs231n/data_utils.py  \n",
            "  inflating: assignment1/cs231n/features.py  \n",
            "  inflating: assignment1/cs231n/gradient_check.py  \n",
            "  inflating: assignment1/cs231n/vis_utils.py  \n",
            " extracting: assignment1/cs231n/__init__.py  \n",
            "   creating: assignment1/cs231n/__pycache__/\n",
            "  inflating: assignment1/cs231n/__pycache__/data_utils.cpython-36.pyc  \n",
            "  inflating: assignment1/cs231n/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: assignment1/features.ipynb  \n",
            "  inflating: assignment1/frameworkpython  \n",
            "  inflating: assignment1/knn.ipynb   \n",
            "  inflating: assignment1/README.md   \n",
            "  inflating: assignment1/requirements.txt  \n",
            "  inflating: assignment1/setup_googlecloud.sh  \n",
            "  inflating: assignment1/softmax.ipynb  \n",
            "  inflating: assignment1/start_ipython_osx.sh  \n",
            "  inflating: assignment1/svm.ipynb   \n",
            "  inflating: assignment1/two_layer_net.ipynb  \n",
            "collectSubmission.sh  knn.ipynb\t\t    softmax.ipynb\n",
            "cs231n\t\t      README.md\t\t    start_ipython_osx.sh\n",
            "features.ipynb\t      requirements.txt\t    svm.ipynb\n",
            "frameworkpython       setup_googlecloud.sh  two_layer_net.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QqUshV9NKfAB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Downloading the dataset"
      ]
    },
    {
      "metadata": {
        "id": "g5RK5geYKgN8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "d02b71dd-6371-4349-8580-b0b68f47eebb"
      },
      "cell_type": "code",
      "source": [
        "!cd assignment1/cs231n/datasets && bash get_datasets.sh && ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-06-28 06:32:37--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar  34%[=====>              ]  55.40M   758KB/s    eta 94s    "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "cifar-10-python.tar 100%[===================>] 162.60M  1.72MB/s    in 1m 58s  \n",
            "\n",
            "2018-06-28 06:34:35 (1.38 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n",
            "cifar-10-batches-py  get_datasets.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0ccrU9bdJqgr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.chdir('assignment1')\n",
        "\n",
        "from cs231n.data_utils import load_CIFAR10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading extenrnal modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwnjbGxaJqg_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ffb6d519-7a44-4788-ccac-8ff76928b5e7"
      },
      "cell_type": "code",
      "source": [
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the linear classifier. These are the same steps as we used for the\n",
        "    SVM, but condensed to a single function.  \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "    \n",
        "    # subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "    X_dev = X_train[mask]\n",
        "    y_dev = y_train[mask]\n",
        "    \n",
        "    # Preprocessing: reshape the image data into rows\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "    \n",
        "    # Normalize the data: subtract the mean image\n",
        "    mean_image = np.mean(X_train, axis = 0)\n",
        "    X_train -= mean_image\n",
        "    X_val -= mean_image\n",
        "    X_test -= mean_image\n",
        "    X_dev -= mean_image\n",
        "    \n",
        "    # add bias dimension and transform into columns\n",
        "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
        "\n",
        "\n",
        "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
        "try:\n",
        "   del X_train, y_train\n",
        "   del X_test, y_test\n",
        "   print('Clear previously loaded data.')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)\n",
        "print('dev data shape: ', X_dev.shape)\n",
        "print('dev labels shape: ', y_dev.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape:  (49000, 3073)\n",
            "Train labels shape:  (49000,)\n",
            "Validation data shape:  (1000, 3073)\n",
            "Validation labels shape:  (1000,)\n",
            "Test data shape:  (1000, 3073)\n",
            "Test labels shape:  (1000,)\n",
            "dev data shape:  (500, 3073)\n",
            "dev labels shape:  (500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jL3B84mZLa7a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Code from the classifiers/softmax.py"
      ]
    },
    {
      "metadata": {
        "id": "jspk6ydJLiqH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "def softmax_loss_naive(W, X, y, reg):\n",
        "  \"\"\"\n",
        "  Softmax loss function, naive implementation (with loops)\n",
        "\n",
        "  Inputs have dimension D, there are C classes, and we operate on minibatches\n",
        "  of N examples.\n",
        "\n",
        "  Inputs:\n",
        "  - W: A numpy array of shape (D, C) containing weights.\n",
        "  - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "    that X[i] has label c, where 0 <= c < C.\n",
        "  - reg: (float) regularization strength\n",
        "\n",
        "  Returns a tuple of:\n",
        "  - loss as single float\n",
        "  - gradient with respect to weights W; an array of same shape as W\n",
        "  \"\"\"\n",
        "  # Initialize the loss and gradient to zero.\n",
        "  loss = 0.0\n",
        "  dW = np.zeros_like(W)\n",
        "\n",
        "  #############################################################################\n",
        "  # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
        "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "  # regularization!                                                           #\n",
        "  #############################################################################\n",
        "  num_train = X.shape[0]\n",
        "  dim = X.shape[1]\n",
        "  num_classes = W.shape[1]\n",
        "  \n",
        "  for i in range(num_train):\n",
        "    scores = X[i].dot(W)\n",
        "    sum_exp_scores = np.sum(np.exp(scores))\n",
        "\n",
        "    for j in range(num_classes):\n",
        "      softmax_j = np.exp(scores[j]) / sum_exp_scores\n",
        "      dW[:, j] += (softmax_j - (j == y[i])) * X[i]\n",
        "    \n",
        "    loss += -scores[y[i]] + np.log(sum_exp_scores)\n",
        "    \n",
        "  loss /= num_train\n",
        "  loss += reg * np.sum(W * W)\n",
        "  \n",
        "  dW /= num_train\n",
        "  dW += reg * W\n",
        "  #############################################################################\n",
        "  #                          END OF YOUR CODE                                 #\n",
        "  #############################################################################\n",
        "\n",
        "  return loss, dW\n",
        "\n",
        "\n",
        "def softmax_loss_vectorized(W, X, y, reg):\n",
        "  \"\"\"\n",
        "  Softmax loss function, vectorized version.\n",
        "\n",
        "  Inputs and outputs are the same as softmax_loss_naive.\n",
        "  \"\"\"\n",
        "  # Initialize the loss and gradient to zero.\n",
        "  loss = 0.0\n",
        "  dW = np.zeros_like(W)\n",
        "\n",
        "  #############################################################################\n",
        "  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "  # regularization!                                                           #\n",
        "  #############################################################################\n",
        "  num_train = X.shape[0]\n",
        "  # scores is a matrix with dimensions CxN, holding the scores of\n",
        "  # images for the various classes\n",
        "  scores = np.dot(W.T, X.T)\n",
        "  exp_scores = np.exp(scores)\n",
        "  sum_exp_scores = np.sum(exp_scores, axis=0)\n",
        "  loss = np.sum( -scores[y, np.arange(num_train)] + np.log(sum_exp_scores) )\n",
        "  \n",
        "  softmax = exp_scores / sum_exp_scores\n",
        "  softmax[y, np.arange(num_train)] -= 1\n",
        "  dW = np.dot(softmax, X)\n",
        "  dW = dW.T\n",
        "  \n",
        "  loss /= num_train\n",
        "  loss += reg * np.sum(W * W)\n",
        "  \n",
        "  dW /= num_train\n",
        "  dW += reg * W\n",
        "  #############################################################################\n",
        "  #                          END OF YOUR CODE                                 #\n",
        "  #############################################################################\n",
        "\n",
        "  return loss, dW\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whvF2bEZJqhN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Softmax Classifier\n"
      ]
    },
    {
      "metadata": {
        "id": "MIf_BSy0JqhQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7f12c8bd-65a7-4582-cf7e-1e9371225c6c"
      },
      "cell_type": "code",
      "source": [
        "# First implement the naive softmax loss function with nested loops.\n",
        "\n",
        "import time\n",
        "\n",
        "# Generate a random softmax weight matrix and use it to compute the loss.\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
        "print('loss: %f' % loss)\n",
        "print('sanity check: %f' % (-np.log(0.1)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 2.361576\n",
            "sanity check: 2.302585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JX0KB-ewJqhe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inline Question 1:\n",
        "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
        "\n",
        "**Your answer:** *Fill this in*\n"
      ]
    },
    {
      "metadata": {
        "id": "J6uVTl_vJqhh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "3ff8e520-f2a4-461d-9df7-0ffeb705f1bc"
      },
      "cell_type": "code",
      "source": [
        "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
        "# version of the gradient that uses nested loops.\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
        "# The numeric gradient should be close to the analytic gradient.\n",
        "from cs231n.gradient_check import grad_check_sparse\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
        "\n",
        "# similar to SVM case, do another gradient check with regularization\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numerical: 0.514033 analytic: 0.514033, relative error: 6.481958e-08\n",
            "numerical: 0.142171 analytic: 0.142171, relative error: 3.379859e-07\n",
            "numerical: 3.151673 analytic: 3.151673, relative error: 7.356590e-09\n",
            "numerical: -1.743162 analytic: -1.743162, relative error: 2.475953e-08\n",
            "numerical: -1.166151 analytic: -1.166151, relative error: 1.055686e-08\n",
            "numerical: -1.259208 analytic: -1.259209, relative error: 3.927110e-08\n",
            "numerical: 2.392345 analytic: 2.392345, relative error: 7.952115e-09\n",
            "numerical: -0.859952 analytic: -0.859952, relative error: 1.956296e-08\n",
            "numerical: -0.040502 analytic: -0.040502, relative error: 1.867296e-06\n",
            "numerical: 1.273227 analytic: 1.273226, relative error: 1.868330e-08\n",
            "numerical: 4.033153 analytic: 4.038752, relative error: 6.936698e-04\n",
            "numerical: -0.036859 analytic: -0.035401, relative error: 2.016670e-02\n",
            "numerical: -1.643878 analytic: -1.643247, relative error: 1.919489e-04\n",
            "numerical: -2.134222 analytic: -2.128865, relative error: 1.256553e-03\n",
            "numerical: -3.740631 analytic: -3.735260, relative error: 7.184430e-04\n",
            "numerical: 0.143164 analytic: 0.136558, relative error: 2.361544e-02\n",
            "numerical: 3.672589 analytic: 3.670616, relative error: 2.686452e-04\n",
            "numerical: -2.434555 analytic: -2.431285, relative error: 6.721017e-04\n",
            "numerical: 2.792426 analytic: 2.798437, relative error: 1.075015e-03\n",
            "numerical: -3.550739 analytic: -3.548262, relative error: 3.489212e-04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GH-qEHQ1Jqho",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "26819fa1-a52d-4ceb-cc0f-f7dba448ec9e"
      },
      "cell_type": "code",
      "source": [
        "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
        "# implement a vectorized version in softmax_loss_vectorized.\n",
        "# The two versions should compute the same results, but the vectorized version should be\n",
        "# much faster.\n",
        "tic = time.time()\n",
        "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
        "\n",
        "tic = time.time()\n",
        "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
        "\n",
        "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
        "# of the gradient.\n",
        "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
        "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
        "print('Gradient difference: %f' % grad_difference)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naive loss: 2.361576e+00 computed in 0.174534s\n",
            "vectorized loss: 2.361576e+00 computed in 0.007149s\n",
            "Loss difference: 0.000000\n",
            "Gradient difference: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0XgJijNnVaF3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Code from the classifiers/linear_classifier.py file"
      ]
    },
    {
      "metadata": {
        "id": "3fzblJnCVvV0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "# from cs231n.classifiers.linear_svm import *\n",
        "# from cs231n.classifiers.softmax import *\n",
        "\n",
        "class LinearClassifier(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.W = None\n",
        "\n",
        "  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "    \"\"\"\n",
        "    Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "      training samples each of dimension D.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "      means that X[i] has label 0 <= c < C for C classes.\n",
        "    - learning_rate: (float) learning rate for optimization.\n",
        "    - reg: (float) regularization strength.\n",
        "    - num_iters: (integer) number of steps to take when optimizing\n",
        "    - batch_size: (integer) number of training examples to use at each step.\n",
        "    - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "    Outputs:\n",
        "    A list containing the value of the loss function at each training iteration.\n",
        "    \"\"\"\n",
        "    num_train, dim = X.shape\n",
        "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
        "    if self.W is None:\n",
        "      # lazily initialize W\n",
        "      self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "    # Run stochastic gradient descent to optimize W\n",
        "    loss_history = []\n",
        "    for it in range(num_iters):\n",
        "      X_batch = None\n",
        "      y_batch = None\n",
        "\n",
        "      #########################################################################\n",
        "      # TODO:                                                                 #\n",
        "      # Sample batch_size elements from the training data and their           #\n",
        "      # corresponding labels to use in this round of gradient descent.        #\n",
        "      # Store the data in X_batch and their corresponding labels in           #\n",
        "      # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
        "      # and y_batch should have shape (batch_size,)                           #\n",
        "      #                                                                       #\n",
        "      # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "      # replacement is faster than sampling without replacement.              #\n",
        "      #########################################################################\n",
        "      indices = np.random.randint(low=0, high=num_train, size=batch_size)\n",
        "      X_batch = X[indices, :]\n",
        "      y_batch = y[indices]\n",
        "      #########################################################################\n",
        "      #                       END OF YOUR CODE                                #\n",
        "      #########################################################################\n",
        "\n",
        "      # evaluate loss and gradient\n",
        "      loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "      loss_history.append(loss)\n",
        "\n",
        "      # perform parameter update\n",
        "      #########################################################################\n",
        "      # TODO:                                                                 #\n",
        "      # Update the weights using the gradient and the learning rate.          #\n",
        "      #########################################################################\n",
        "      self.W += -learning_rate * grad\n",
        "      #########################################################################\n",
        "      #                       END OF YOUR CODE                                #\n",
        "      #########################################################################\n",
        "\n",
        "      if verbose and it % 100 == 0:\n",
        "        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "    return loss_history\n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Use the trained weights of this linear classifier to predict labels for\n",
        "    data points.\n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "      training samples each of dimension D.\n",
        "\n",
        "    Returns:\n",
        "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "      array of length N, and each element is an integer giving the predicted\n",
        "      class.\n",
        "    \"\"\"\n",
        "    y_pred = np.zeros(X.shape[0])\n",
        "    ###########################################################################\n",
        "    # TODO:                                                                   #\n",
        "    # Implement this method. Store the predicted labels in y_pred.            #\n",
        "    ###########################################################################\n",
        "    prod = np.dot(self.W.T, X.T)     # prod's dimensions: CxN\n",
        "    y_pred = np.argmax(prod, axis=0)\n",
        "    ###########################################################################\n",
        "    #                           END OF YOUR CODE                              #\n",
        "    ###########################################################################\n",
        "    return y_pred\n",
        "  \n",
        "  def loss(self, X_batch, y_batch, reg):\n",
        "    \"\"\"\n",
        "    Compute the loss function and its derivative. \n",
        "    Subclasses will override this.\n",
        "\n",
        "    Inputs:\n",
        "    - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "      data points; each point has dimension D.\n",
        "    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "    - reg: (float) regularization strength.\n",
        "\n",
        "    Returns: A tuple containing:\n",
        "    - loss as a single float\n",
        "    - gradient with respect to self.W; an array of the same shape as W\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "class LinearSVM(LinearClassifier):\n",
        "  \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
        "\n",
        "  def loss(self, X_batch, y_batch, reg):\n",
        "    return svm_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
        "\n",
        "\n",
        "class Softmax(LinearClassifier):\n",
        "  \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
        "\n",
        "  def loss(self, X_batch, y_batch, reg):\n",
        "    return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hq7U-wcoV5ci",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tune hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "EWBZcwycJqh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1128
        },
        "outputId": "6fdb235a-6a7d-43e7-82a6-356c0a4a771b"
      },
      "cell_type": "code",
      "source": [
        "# Use the validation set to tune hyperparameters (regularization strength and\n",
        "# learning rate). You should experiment with different ranges for the learning\n",
        "# rates and regularization strengths; if you are careful you should be able to\n",
        "# get a classification accuracy of over 0.35 on the validation set.\n",
        "# from cs231n.classifiers import Softmax\n",
        "results = {}\n",
        "best_val = -1\n",
        "best_softmax = None\n",
        "learning_rates = [3.39e-7, 3.4e-7]\n",
        "regularization_strengths = [2.79e4, 2.80e4]\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Use the validation set to set the learning rate and regularization strength. #\n",
        "# This should be identical to the validation that you did for the SVM; save    #\n",
        "# the best trained softmax classifer in best_softmax.                          #\n",
        "################################################################################\n",
        "for i in range(0, 3):\n",
        "    print(\"\\nIteration no.: %d\\n\" % i)\n",
        "    # sample a learning rate and reg rate from the given range\n",
        "    learning_rate = (learning_rates[1] - learning_rates[0]) \\\n",
        "                    * np.random.random_sample() + learning_rates[0]\n",
        "    reg = (regularization_strengths[1] - regularization_strengths[0]) \\\n",
        "          * np.random.random_sample() + regularization_strengths[0]\n",
        "    print(\"LEARNING RATE: %e\" % learning_rate)\n",
        "    print(\"REG STRENGTH: %e\" % reg)\n",
        "    softmax = Softmax()\n",
        "    loss_hist = softmax.train(X_train, y_train, learning_rate=learning_rate,\n",
        "                      reg=reg, num_iters=1500, verbose=True)\n",
        "    final_loss = loss_hist[-1]\n",
        "    \n",
        "    # predict values\n",
        "    y_train_pred = softmax.predict(X_train)\n",
        "    training_accuracy = np.mean(y_train == y_train_pred)\n",
        "    y_val_pred = softmax.predict(X_val)\n",
        "    validation_accuracy = np.mean(y_val == y_val_pred)\n",
        "    \n",
        "    # store the data in results\n",
        "    results[(learning_rate, reg)] = (training_accuracy, validation_accuracy)\n",
        "    \n",
        "    # Check if the result we got is the best one we've achieved until now\n",
        "    if validation_accuracy > best_val:\n",
        "        best_val = validation_accuracy\n",
        "        best_softmax = softmax\n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "    \n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "    \n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration no.: 0\n",
            "\n",
            "LEARNING RATE: 3.393962e-07\n",
            "REG STRENGTH: 2.799060e+04\n",
            "iteration 0 / 1500: loss 875.340297\n",
            "iteration 100 / 1500: loss 130.265663\n",
            "iteration 200 / 1500: loss 21.005524\n",
            "iteration 300 / 1500: loss 4.948499\n",
            "iteration 400 / 1500: loss 2.595785\n",
            "iteration 500 / 1500: loss 2.170209\n",
            "iteration 600 / 1500: loss 2.137246\n",
            "iteration 700 / 1500: loss 2.098557\n",
            "iteration 800 / 1500: loss 2.115673\n",
            "iteration 900 / 1500: loss 2.085254\n",
            "iteration 1000 / 1500: loss 2.154884\n",
            "iteration 1100 / 1500: loss 2.132630\n",
            "iteration 1200 / 1500: loss 2.167437\n",
            "iteration 1300 / 1500: loss 2.111220\n",
            "iteration 1400 / 1500: loss 2.052013\n",
            "\n",
            "Iteration no.: 1\n",
            "\n",
            "LEARNING RATE: 3.393609e-07\n",
            "REG STRENGTH: 2.796134e+04\n",
            "iteration 0 / 1500: loss 857.414745\n",
            "iteration 100 / 1500: loss 127.614399\n",
            "iteration 200 / 1500: loss 20.632800\n",
            "iteration 300 / 1500: loss 4.912106\n",
            "iteration 400 / 1500: loss 2.528693\n",
            "iteration 500 / 1500: loss 2.232088\n",
            "iteration 600 / 1500: loss 2.123005\n",
            "iteration 700 / 1500: loss 2.096329\n",
            "iteration 800 / 1500: loss 2.105287\n",
            "iteration 900 / 1500: loss 2.096640\n",
            "iteration 1000 / 1500: loss 2.104143\n",
            "iteration 1100 / 1500: loss 2.137169\n",
            "iteration 1200 / 1500: loss 2.094307\n",
            "iteration 1300 / 1500: loss 2.034205\n",
            "iteration 1400 / 1500: loss 2.126393\n",
            "\n",
            "Iteration no.: 2\n",
            "\n",
            "LEARNING RATE: 3.390353e-07\n",
            "REG STRENGTH: 2.797119e+04\n",
            "iteration 0 / 1500: loss 869.990956\n",
            "iteration 100 / 1500: loss 129.748449\n",
            "iteration 200 / 1500: loss 21.028675\n",
            "iteration 300 / 1500: loss 4.934804\n",
            "iteration 400 / 1500: loss 2.579660\n",
            "iteration 500 / 1500: loss 2.133766\n",
            "iteration 600 / 1500: loss 2.076174\n",
            "iteration 700 / 1500: loss 2.104316\n",
            "iteration 800 / 1500: loss 2.102458\n",
            "iteration 900 / 1500: loss 2.134333\n",
            "iteration 1000 / 1500: loss 2.058890\n",
            "iteration 1100 / 1500: loss 2.144773\n",
            "iteration 1200 / 1500: loss 2.117833\n",
            "iteration 1300 / 1500: loss 2.080107\n",
            "iteration 1400 / 1500: loss 2.059329\n",
            "lr 3.390353e-07 reg 2.797119e+04 train accuracy: 0.348163 val accuracy: 0.363000\n",
            "lr 3.393609e-07 reg 2.796134e+04 train accuracy: 0.340796 val accuracy: 0.355000\n",
            "lr 3.393962e-07 reg 2.799060e+04 train accuracy: 0.343265 val accuracy: 0.351000\n",
            "best validation accuracy achieved during cross-validation: 0.363000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tyo11a1WJqiF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f87d7a15-36b8-4111-94a4-39446b1e9ad4"
      },
      "cell_type": "code",
      "source": [
        "# evaluate on test set\n",
        "# Evaluate the best softmax on test set\n",
        "y_test_pred = best_softmax.predict(X_test)\n",
        "test_accuracy = np.mean(y_test == y_test_pred)\n",
        "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax on raw pixels final test set accuracy: 0.359000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_m2zUezMJqiY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Inline Question** - *True or False*\n",
        "\n",
        "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
        "\n",
        "*Your answer*:\n",
        "\n",
        "*Your explanation*:"
      ]
    },
    {
      "metadata": {
        "id": "G0q-oidFJqic",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "0cbae35b-996b-4bcb-b54c-489836335d48"
      },
      "cell_type": "code",
      "source": [
        "# Visualize the learned weights for each class\n",
        "w = best_softmax.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    \n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAF7CAYAAAAkBgR2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/i/mLos89Vj0EgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f68cf82aba8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}