{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "softmax.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kyezil/cs231n-solutions/blob/master/assignment1/softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "kQqzyo-yJqgj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Softmax exercise\n",
        "\n",
        "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
        "\n",
        "This exercise is analogous to the SVM exercise. You will:\n",
        "\n",
        "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
        "- implement the fully-vectorized expression for its **analytic gradient**\n",
        "- **check your implementation** with numerical gradient\n",
        "- use a validation set to **tune the learning rate and regularization** strength\n",
        "- **optimize** the loss function with **SGD**\n",
        "- **visualize** the final learned weights\n"
      ]
    },
    {
      "metadata": {
        "id": "-0njHvKxKGKP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### To cross check if notebook is running on GPU backend"
      ]
    },
    {
      "metadata": {
        "id": "p68ra1TrKOYm",
        "colab_type": "code",
        "outputId": "c19fd8ea-71a3-4040-9bbd-1096a8cf34d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "53XjkRggKRXN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Download and extract the required files"
      ]
    },
    {
      "metadata": {
        "id": "M-4D-PirKVmU",
        "colab_type": "code",
        "outputId": "b53c401a-249d-47f3-eec7-72767fa6738a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -rf assignment1\n",
        "!wget http://cs231n.github.io/assignments/2018/spring1718_assignment1.zip\n",
        "!unzip spring1718_assignment1.zip\n",
        "!rm spring1718_assignment1.zip\n",
        "!cd ./assignment1 && ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-15 20:52:43--  http://cs231n.github.io/assignments/2018/spring1718_assignment1.zip\n",
            "Resolving cs231n.github.io (cs231n.github.io)... 185.199.110.153, 185.199.108.153, 185.199.111.153, ...\n",
            "Connecting to cs231n.github.io (cs231n.github.io)|185.199.110.153|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73256 (72K) [application/zip]\n",
            "Saving to: ‘spring1718_assignment1.zip’\n",
            "\n",
            "spring1718_assignme 100%[===================>]  71.54K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2019-02-15 20:52:44 (2.88 MB/s) - ‘spring1718_assignment1.zip’ saved [73256/73256]\n",
            "\n",
            "Archive:  spring1718_assignment1.zip\n",
            "   creating: assignment1/\n",
            " extracting: assignment1/.gitignore  \n",
            "   creating: assignment1/.ipynb_checkpoints/\n",
            "  inflating: assignment1/.ipynb_checkpoints/features-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/knn-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/softmax-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/svm-checkpoint.ipynb  \n",
            "  inflating: assignment1/.ipynb_checkpoints/two_layer_net-checkpoint.ipynb  \n",
            "  inflating: assignment1/collectSubmission.sh  \n",
            "   creating: assignment1/cs231n/\n",
            "   creating: assignment1/cs231n/classifiers/\n",
            "  inflating: assignment1/cs231n/classifiers/k_nearest_neighbor.py  \n",
            "  inflating: assignment1/cs231n/classifiers/linear_classifier.py  \n",
            "  inflating: assignment1/cs231n/classifiers/linear_svm.py  \n",
            "  inflating: assignment1/cs231n/classifiers/neural_net.py  \n",
            "  inflating: assignment1/cs231n/classifiers/softmax.py  \n",
            "  inflating: assignment1/cs231n/classifiers/__init__.py  \n",
            "   creating: assignment1/cs231n/datasets/\n",
            "  inflating: assignment1/cs231n/datasets/.gitignore  \n",
            "  inflating: assignment1/cs231n/datasets/get_datasets.sh  \n",
            "  inflating: assignment1/cs231n/data_utils.py  \n",
            "  inflating: assignment1/cs231n/features.py  \n",
            "  inflating: assignment1/cs231n/gradient_check.py  \n",
            "  inflating: assignment1/cs231n/vis_utils.py  \n",
            " extracting: assignment1/cs231n/__init__.py  \n",
            "   creating: assignment1/cs231n/__pycache__/\n",
            "  inflating: assignment1/cs231n/__pycache__/data_utils.cpython-36.pyc  \n",
            "  inflating: assignment1/cs231n/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: assignment1/features.ipynb  \n",
            "  inflating: assignment1/frameworkpython  \n",
            "  inflating: assignment1/knn.ipynb   \n",
            "  inflating: assignment1/README.md   \n",
            "  inflating: assignment1/requirements.txt  \n",
            "  inflating: assignment1/setup_googlecloud.sh  \n",
            "  inflating: assignment1/softmax.ipynb  \n",
            "  inflating: assignment1/start_ipython_osx.sh  \n",
            "  inflating: assignment1/svm.ipynb   \n",
            "  inflating: assignment1/two_layer_net.ipynb  \n",
            "collectSubmission.sh  knn.ipynb\t\t    softmax.ipynb\n",
            "cs231n\t\t      README.md\t\t    start_ipython_osx.sh\n",
            "features.ipynb\t      requirements.txt\t    svm.ipynb\n",
            "frameworkpython       setup_googlecloud.sh  two_layer_net.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QqUshV9NKfAB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Downloading the dataset"
      ]
    },
    {
      "metadata": {
        "id": "g5RK5geYKgN8",
        "colab_type": "code",
        "outputId": "a054b7da-fa04-42cc-9a73-d94df6b1cd85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "cell_type": "code",
      "source": [
        "!cd assignment1/cs231n/datasets && bash get_datasets.sh && ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-02-15 20:52:51--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  1.39MB/s    in 72s     \n",
            "\n",
            "2019-02-15 20:54:04 (2.25 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n",
            "cifar-10-batches-py  get_datasets.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0ccrU9bdJqgr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.chdir('assignment1')\n",
        "\n",
        "from cs231n.data_utils import load_CIFAR10\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading extenrnal modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qwnjbGxaJqg_",
        "colab_type": "code",
        "outputId": "2f59e2d9-90b6-451a-edb1-71eb9d5fd1c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for the linear classifier. These are the same steps as we used for the\n",
        "    SVM, but condensed to a single function.  \n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
        "    \n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "    \n",
        "    # subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
        "    X_dev = X_train[mask]\n",
        "    y_dev = y_train[mask]\n",
        "    \n",
        "    # Preprocessing: reshape the image data into rows\n",
        "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
        "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
        "    \n",
        "    # Normalize the data: subtract the mean image\n",
        "    mean_image = np.mean(X_train, axis = 0)\n",
        "    X_train -= mean_image\n",
        "    X_val -= mean_image\n",
        "    X_test -= mean_image\n",
        "    X_dev -= mean_image\n",
        "    \n",
        "    # add bias dimension and transform into columns\n",
        "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
        "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
        "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
        "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
        "    \n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
        "\n",
        "\n",
        "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
        "try:\n",
        "   del X_train, y_train\n",
        "   del X_test, y_test\n",
        "   print('Clear previously loaded data.')\n",
        "except:\n",
        "   pass\n",
        "\n",
        "# Invoke the above function to get our data.\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
        "print('Train data shape: ', X_train.shape)\n",
        "print('Train labels shape: ', y_train.shape)\n",
        "print('Validation data shape: ', X_val.shape)\n",
        "print('Validation labels shape: ', y_val.shape)\n",
        "print('Test data shape: ', X_test.shape)\n",
        "print('Test labels shape: ', y_test.shape)\n",
        "print('dev data shape: ', X_dev.shape)\n",
        "print('dev labels shape: ', y_dev.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data shape:  (49000, 3073)\n",
            "Train labels shape:  (49000,)\n",
            "Validation data shape:  (1000, 3073)\n",
            "Validation labels shape:  (1000,)\n",
            "Test data shape:  (1000, 3073)\n",
            "Test labels shape:  (1000,)\n",
            "dev data shape:  (500, 3073)\n",
            "dev labels shape:  (500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jL3B84mZLa7a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Code from the classifiers/softmax.py"
      ]
    },
    {
      "metadata": {
        "id": "jspk6ydJLiqH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "def softmax_loss_naive(W, X, y, reg):\n",
        "  \"\"\"\n",
        "  Softmax loss function, naive implementation (with loops)\n",
        "\n",
        "  Inputs have dimension D, there are C classes, and we operate on minibatches\n",
        "  of N examples.\n",
        "\n",
        "  Inputs:\n",
        "  - W: A numpy array of shape (D, C) containing weights.\n",
        "  - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
        "  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n",
        "    that X[i] has label c, where 0 <= c < C.\n",
        "  - reg: (float) regularization strength\n",
        "\n",
        "  Returns a tuple of:\n",
        "  - loss as single float\n",
        "  - gradient with respect to weights W; an array of same shape as W\n",
        "  \"\"\"\n",
        "  # Initialize the loss and gradient to zero.\n",
        "  loss = 0.0\n",
        "  dW = np.zeros_like(W)\n",
        "\n",
        "  N = X.shape[0]\n",
        "  C = W.shape[1]\n",
        "  #############################################################################\n",
        "  # TODO: Compute the softmax loss and its gradient using explicit loops.     #\n",
        "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "  # regularization!                                                           #\n",
        "  #############################################################################\n",
        "  for i in range(N):\n",
        "    f = X[i].dot(W)\n",
        "    f -= np.max(f) # numeric stability\n",
        "    ef = np.exp(f)\n",
        "    sef = np.sum(ef)\n",
        "    loss += -f[y[i]] + np.log(sef)\n",
        "    dW[:,y[i]] -= X[i]\n",
        "    for j in range(C):\n",
        "      dW[:,j] += ef[j]/sef * X[i]\n",
        "    \n",
        "  loss /= N\n",
        "  dW /= N\n",
        "  # regularization\n",
        "  loss += reg * np.sum(W*W)\n",
        "  dW += reg * 2 * W\n",
        "  #############################################################################\n",
        "  #                          END OF YOUR CODE                                 #\n",
        "  #############################################################################\n",
        "  \n",
        "  return loss, dW\n",
        "\n",
        "\n",
        "def softmax_loss_vectorized(W, X, y, reg):\n",
        "  \"\"\"\n",
        "  Softmax loss function, vectorized version.\n",
        "\n",
        "  Inputs and outputs are the same as softmax_loss_naive.\n",
        "  \"\"\"\n",
        "  # Initialize the loss and gradient to zero.\n",
        "  loss = 0.0\n",
        "  dW = np.zeros_like(W)\n",
        "\n",
        "  N = X.shape[0]\n",
        "  C = W.shape[1]\n",
        "  #############################################################################\n",
        "  # TODO: Compute the softmax loss and its gradient using no explicit loops.  #\n",
        "  # Store the loss in loss and the gradient in dW. If you are not careful     #\n",
        "  # here, it is easy to run into numeric instability. Don't forget the        #\n",
        "  # regularization!                                                           #\n",
        "  #############################################################################\n",
        "  F = X @ W # (N,C) matrix\n",
        "  F -= np.max(F, axis=1, keepdims=True) # remove (N,1) column wise\n",
        "  # keepdims makes shape be (N,1) instead of (1,N)  \n",
        "  F = np.exp(F)\n",
        "  F = np.divide(F, np.sum(F, axis=1, keepdims=True))\n",
        "  # Fij = F[i][j] / sum_j F[i][j]\n",
        "  \n",
        "  loss -= np.log(F[np.arange(N), y]).sum()\n",
        "  \n",
        "  F[np.arange(N),y] -= 1 # -f_y[i]\n",
        "  dW += X.T @ F\n",
        "  \n",
        "  loss /= N\n",
        "  dW /= N\n",
        "  \n",
        "  # regularization\n",
        "  loss += 2 * reg * np.sum(W*W)\n",
        "  dW += reg * 2 * W\n",
        "  #############################################################################\n",
        "  #                          END OF YOUR CODE                                 #\n",
        "  #############################################################################\n",
        "\n",
        "  return loss, dW\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "whvF2bEZJqhN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Softmax Classifier\n"
      ]
    },
    {
      "metadata": {
        "id": "MIf_BSy0JqhQ",
        "colab_type": "code",
        "outputId": "59b28e01-b9e8-42e6-dbaf-bb072cd56f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# First implement the naive softmax loss function with nested loops.\n",
        "\n",
        "import time\n",
        "\n",
        "# Generate a random softmax weight matrix and use it to compute the loss.\n",
        "W = np.random.randn(3073, 10) * 0.0001\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
        "print('loss: %f' % loss)\n",
        "print('sanity check: %f' % (-np.log(0.1)))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss: 2.371791\n",
            "sanity check: 2.302585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JX0KB-ewJqhe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inline Question 1:\n",
        "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
        "\n",
        "**Your answer:** as W ~= 0,  f ~= 0, then for each sample the loss is ~=  -log(1/C) and C is 10 here, regularization is ~ 0^2\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "J6uVTl_vJqhh",
        "colab_type": "code",
        "outputId": "e97cc592-7c11-4f19-87b0-1ca99e442752",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
        "# version of the gradient that uses nested loops.\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
        "\n",
        "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
        "# The numeric gradient should be close to the analytic gradient.\n",
        "from cs231n.gradient_check import grad_check_sparse\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
        "\n",
        "# similar to SVM case, do another gradient check with regularization\n",
        "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
        "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
        "grad_numerical = grad_check_sparse(f, W, grad, 10)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "numerical: 1.870630 analytic: 1.870630, relative error: 2.867572e-08\n",
            "numerical: 1.838871 analytic: 1.838871, relative error: 2.195981e-08\n",
            "numerical: 1.807605 analytic: 1.807605, relative error: 2.694861e-08\n",
            "numerical: 1.389669 analytic: 1.389669, relative error: 1.183589e-08\n",
            "numerical: 1.848576 analytic: 1.848576, relative error: 2.709597e-08\n",
            "numerical: -1.702481 analytic: -1.702481, relative error: 4.797211e-09\n",
            "numerical: 1.539867 analytic: 1.539866, relative error: 3.496111e-08\n",
            "numerical: 0.166888 analytic: 0.166888, relative error: 3.058189e-09\n",
            "numerical: 1.571030 analytic: 1.571030, relative error: 4.624316e-08\n",
            "numerical: -0.143949 analytic: -0.143949, relative error: 1.393184e-07\n",
            "numerical: 3.801120 analytic: 3.801120, relative error: 1.765147e-08\n",
            "numerical: 4.628067 analytic: 4.628067, relative error: 1.590048e-08\n",
            "numerical: 1.254478 analytic: 1.254478, relative error: 8.406982e-09\n",
            "numerical: -0.883820 analytic: -0.883820, relative error: 4.945988e-08\n",
            "numerical: -1.544730 analytic: -1.544730, relative error: 3.510871e-09\n",
            "numerical: 0.189210 analytic: 0.189210, relative error: 1.278481e-07\n",
            "numerical: -0.595983 analytic: -0.595983, relative error: 2.022521e-08\n",
            "numerical: 0.594272 analytic: 0.594272, relative error: 1.063540e-07\n",
            "numerical: -2.503536 analytic: -2.503536, relative error: 1.328185e-08\n",
            "numerical: 0.683488 analytic: 0.683488, relative error: 9.314424e-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GH-qEHQ1Jqho",
        "colab_type": "code",
        "outputId": "1a584805-0347-48cb-820d-f665aa4b7900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
        "# implement a vectorized version in softmax_loss_vectorized.\n",
        "# The two versions should compute the same results, but the vectorized version should be\n",
        "# much faster.\n",
        "tic = time.time()\n",
        "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
        "\n",
        "tic = time.time()\n",
        "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
        "toc = time.time()\n",
        "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
        "\n",
        "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
        "# of the gradient.\n",
        "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
        "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
        "print('Gradient difference: %f' % grad_difference)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "naive loss: 2.371791e+00 computed in 0.191880s\n",
            "vectorized loss: 2.371791e+00 computed in 0.013008s\n",
            "Loss difference: 0.000000\n",
            "Gradient difference: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0XgJijNnVaF3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Code from the classifiers/linear_classifier.py file"
      ]
    },
    {
      "metadata": {
        "id": "3fzblJnCVvV0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class LinearClassifier(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.W = None\n",
        "\n",
        "  def train(self, X, y, learning_rate=1e-3, reg=1e-5, num_iters=100,\n",
        "            batch_size=200, verbose=False):\n",
        "    \"\"\"\n",
        "    Train this linear classifier using stochastic gradient descent.\n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "      training samples each of dimension D.\n",
        "    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n",
        "      means that X[i] has label 0 <= c < C for C classes.\n",
        "    - learning_rate: (float) learning rate for optimization.\n",
        "    - reg: (float) regularization strength.\n",
        "    - num_iters: (integer) number of steps to take when optimizing\n",
        "    - batch_size: (integer) number of training examples to use at each step.\n",
        "    - verbose: (boolean) If true, print progress during optimization.\n",
        "\n",
        "    Outputs:\n",
        "    A list containing the value of the loss function at each training iteration.\n",
        "    \"\"\"\n",
        "    num_train, dim = X.shape\n",
        "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
        "    if self.W is None:\n",
        "      # lazily initialize W\n",
        "      self.W = 0.001 * np.random.randn(dim, num_classes)\n",
        "\n",
        "    # Run stochastic gradient descent to optimize W\n",
        "    loss_history = []\n",
        "    for it in range(num_iters):\n",
        "      X_batch = None\n",
        "      y_batch = None\n",
        "\n",
        "      #########################################################################\n",
        "      # TODO:                                                                 #\n",
        "      # Sample batch_size elements from the training data and their           #\n",
        "      # corresponding labels to use in this round of gradient descent.        #\n",
        "      # Store the data in X_batch and their corresponding labels in           #\n",
        "      # y_batch; after sampling X_batch should have shape (dim, batch_size)   #\n",
        "      # and y_batch should have shape (batch_size,)                           #\n",
        "      #                                                                       #\n",
        "      # Hint: Use np.random.choice to generate indices. Sampling with         #\n",
        "      # replacement is faster than sampling without replacement.              #\n",
        "      #########################################################################\n",
        "      idx = np.random.choice(np.arange(num_train), batch_size)\n",
        "      X_batch = X[idx]\n",
        "      y_batch = y[idx]\n",
        "      #########################################################################\n",
        "      #                       END OF YOUR CODE                                #\n",
        "      #########################################################################\n",
        "\n",
        "      # evaluate loss and gradient\n",
        "      loss, grad = self.loss(X_batch, y_batch, reg)\n",
        "      loss_history.append(loss)\n",
        "\n",
        "      # perform parameter update\n",
        "      #########################################################################\n",
        "      # TODO:                                                                 #\n",
        "      # Update the weights using the gradient and the learning rate.          #\n",
        "      #########################################################################\n",
        "      self.W += - learning_rate * grad\n",
        "      #########################################################################\n",
        "      #                       END OF YOUR CODE                                #\n",
        "      #########################################################################\n",
        "\n",
        "      if verbose and it % 100 == 0:\n",
        "        print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
        "\n",
        "    return loss_history\n",
        "  \n",
        "\n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Use the trained weights of this linear classifier to predict labels for\n",
        "    data points.\n",
        "\n",
        "    Inputs:\n",
        "    - X: A numpy array of shape (N, D) containing training data; there are N\n",
        "      training samples each of dimension D.\n",
        "\n",
        "    Returns:\n",
        "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
        "      array of length N, and each element is an integer giving the predicted\n",
        "      class.\n",
        "    \"\"\"\n",
        "    y_pred = np.zeros(X.shape[0])\n",
        "    ###########################################################################\n",
        "    # TODO:                                                                   #\n",
        "    # Implement this method. Store the predicted labels in y_pred.            #\n",
        "    ###########################################################################\n",
        "    weights = X.dot(self.W) # (N,D)x(D,C) = (N,C)\n",
        "    y_pred = np.argmax(weights, axis=1)\n",
        "    ###########################################################################\n",
        "    #                           END OF YOUR CODE                              #\n",
        "    ###########################################################################\n",
        "    return y_pred\n",
        "  \n",
        "  def loss(self, X_batch, y_batch, reg):\n",
        "    \"\"\"\n",
        "    Compute the loss function and its derivative. \n",
        "    Subclasses will override this.\n",
        "\n",
        "    Inputs:\n",
        "    - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
        "      data points; each point has dimension D.\n",
        "    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
        "    - reg: (float) regularization strength.\n",
        "\n",
        "    Returns: A tuple containing:\n",
        "    - loss as a single float\n",
        "    - gradient with respect to self.W; an array of the same shape as W\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class LinearSVM(LinearClassifier):\n",
        "  \"\"\" A subclass that uses the Multiclass SVM loss function \"\"\"\n",
        "\n",
        "  def loss(self, X_batch, y_batch, reg):\n",
        "    return svm_loss_vectorized(self.W, X_batch, y_batch, reg)\n",
        "\n",
        "\n",
        "class Softmax(LinearClassifier):\n",
        "  \"\"\" A subclass that uses the Softmax + Cross-entropy loss function \"\"\"\n",
        "\n",
        "  def loss(self, X_batch, y_batch, reg):\n",
        "    return softmax_loss_vectorized(self.W, X_batch, y_batch, reg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hq7U-wcoV5ci",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tune hyperparameters"
      ]
    },
    {
      "metadata": {
        "id": "EWBZcwycJqh2",
        "colab_type": "code",
        "outputId": "6e324c2c-9b1e-4372-f691-8a457801cf6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6981
        }
      },
      "cell_type": "code",
      "source": [
        "# Use the validation set to tune hyperparameters (regularization strength and\n",
        "# learning rate). You should experiment with different ranges for the learning\n",
        "# rates and regularization strengths; if you are careful you should be able to\n",
        "# get a classification accuracy of over 0.35 on the validation set.\n",
        "# from cs231n.classifiers import Softmax\n",
        "results = {}\n",
        "best_val = -1\n",
        "best_softmax = None\n",
        "learning_rates = np.logspace(-8,-5,5)\n",
        "regularization_strengths = np.logspace(3,6,5)\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Use the validation set to set the learning rate and regularization strength. #\n",
        "# This should be identical to the validation that you did for the SVM; save    #\n",
        "# the best trained softmax classifer in best_softmax.                          #\n",
        "################################################################################\n",
        "\n",
        "# from svm\n",
        "for lr in learning_rates:\n",
        "  for rs in regularization_strengths:\n",
        "    model = Softmax()\n",
        "    loss_hist = model.train(X_train, y_train, learning_rate=1e-7, reg=2.5e4,\n",
        "                      num_iters=1500, verbose=True)\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    train_acc = np.mean(y_train == y_train_pred)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    val_acc = np.mean(y_val == y_val_pred)\n",
        "    if val_acc > best_val:\n",
        "      best_val = val_acc\n",
        "      best_softmax = model\n",
        "    results[(lr,rs)] = train_acc, val_acc\n",
        "    \n",
        "################################################################################\n",
        "#                              END OF YOUR CODE                                #\n",
        "################################################################################\n",
        "    \n",
        "# Print out results.\n",
        "for lr, reg in sorted(results):\n",
        "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
        "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
        "                lr, reg, train_accuracy, val_accuracy))\n",
        "    \n",
        "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iteration 0 / 1500: loss 1533.113263\n",
            "iteration 100 / 1500: loss 561.651268\n",
            "iteration 200 / 1500: loss 207.004027\n",
            "iteration 300 / 1500: loss 77.124675\n",
            "iteration 400 / 1500: loss 29.636232\n",
            "iteration 500 / 1500: loss 12.204223\n",
            "iteration 600 / 1500: loss 5.801865\n",
            "iteration 700 / 1500: loss 3.547341\n",
            "iteration 800 / 1500: loss 2.669916\n",
            "iteration 900 / 1500: loss 2.345226\n",
            "iteration 1000 / 1500: loss 2.249116\n",
            "iteration 1100 / 1500: loss 2.232582\n",
            "iteration 1200 / 1500: loss 2.226248\n",
            "iteration 1300 / 1500: loss 2.169220\n",
            "iteration 1400 / 1500: loss 2.141703\n",
            "iteration 0 / 1500: loss 1547.025759\n",
            "iteration 100 / 1500: loss 566.889873\n",
            "iteration 200 / 1500: loss 208.806242\n",
            "iteration 300 / 1500: loss 77.808598\n",
            "iteration 400 / 1500: loss 29.902929\n",
            "iteration 500 / 1500: loss 12.337544\n",
            "iteration 600 / 1500: loss 5.940370\n",
            "iteration 700 / 1500: loss 3.464721\n",
            "iteration 800 / 1500: loss 2.679211\n",
            "iteration 900 / 1500: loss 2.358817\n",
            "iteration 1000 / 1500: loss 2.275385\n",
            "iteration 1100 / 1500: loss 2.252461\n",
            "iteration 1200 / 1500: loss 2.145736\n",
            "iteration 1300 / 1500: loss 2.192685\n",
            "iteration 1400 / 1500: loss 2.143996\n",
            "iteration 0 / 1500: loss 1535.309431\n",
            "iteration 100 / 1500: loss 563.159744\n",
            "iteration 200 / 1500: loss 207.510157\n",
            "iteration 300 / 1500: loss 77.290870\n",
            "iteration 400 / 1500: loss 29.765226\n",
            "iteration 500 / 1500: loss 12.157014\n",
            "iteration 600 / 1500: loss 5.911613\n",
            "iteration 700 / 1500: loss 3.471901\n",
            "iteration 800 / 1500: loss 2.667920\n",
            "iteration 900 / 1500: loss 2.361698\n",
            "iteration 1000 / 1500: loss 2.240663\n",
            "iteration 1100 / 1500: loss 2.205116\n",
            "iteration 1200 / 1500: loss 2.165492\n",
            "iteration 1300 / 1500: loss 2.125665\n",
            "iteration 1400 / 1500: loss 2.210680\n",
            "iteration 0 / 1500: loss 1534.634367\n",
            "iteration 100 / 1500: loss 562.129694\n",
            "iteration 200 / 1500: loss 207.095805\n",
            "iteration 300 / 1500: loss 77.135364\n",
            "iteration 400 / 1500: loss 29.568945\n",
            "iteration 500 / 1500: loss 12.190356\n",
            "iteration 600 / 1500: loss 5.843503\n",
            "iteration 700 / 1500: loss 3.535134\n",
            "iteration 800 / 1500: loss 2.689760\n",
            "iteration 900 / 1500: loss 2.337442\n",
            "iteration 1000 / 1500: loss 2.248687\n",
            "iteration 1100 / 1500: loss 2.209293\n",
            "iteration 1200 / 1500: loss 2.175045\n",
            "iteration 1300 / 1500: loss 2.132607\n",
            "iteration 1400 / 1500: loss 2.167576\n",
            "iteration 0 / 1500: loss 1537.639968\n",
            "iteration 100 / 1500: loss 563.712310\n",
            "iteration 200 / 1500: loss 207.638620\n",
            "iteration 300 / 1500: loss 77.435042\n",
            "iteration 400 / 1500: loss 29.734021\n",
            "iteration 500 / 1500: loss 12.282090\n",
            "iteration 600 / 1500: loss 5.897631\n",
            "iteration 700 / 1500: loss 3.529095\n",
            "iteration 800 / 1500: loss 2.620821\n",
            "iteration 900 / 1500: loss 2.383098\n",
            "iteration 1000 / 1500: loss 2.190189\n",
            "iteration 1100 / 1500: loss 2.164771\n",
            "iteration 1200 / 1500: loss 2.152151\n",
            "iteration 1300 / 1500: loss 2.144423\n",
            "iteration 1400 / 1500: loss 2.144845\n",
            "iteration 0 / 1500: loss 1540.003240\n",
            "iteration 100 / 1500: loss 564.209205\n",
            "iteration 200 / 1500: loss 207.823528\n",
            "iteration 300 / 1500: loss 77.425549\n",
            "iteration 400 / 1500: loss 29.755511\n",
            "iteration 500 / 1500: loss 12.263417\n",
            "iteration 600 / 1500: loss 5.924186\n",
            "iteration 700 / 1500: loss 3.554132\n",
            "iteration 800 / 1500: loss 2.617402\n",
            "iteration 900 / 1500: loss 2.331415\n",
            "iteration 1000 / 1500: loss 2.251748\n",
            "iteration 1100 / 1500: loss 2.186647\n",
            "iteration 1200 / 1500: loss 2.187505\n",
            "iteration 1300 / 1500: loss 2.240394\n",
            "iteration 1400 / 1500: loss 2.197138\n",
            "iteration 0 / 1500: loss 1538.021521\n",
            "iteration 100 / 1500: loss 563.078336\n",
            "iteration 200 / 1500: loss 207.284533\n",
            "iteration 300 / 1500: loss 77.212794\n",
            "iteration 400 / 1500: loss 29.681261\n",
            "iteration 500 / 1500: loss 12.228952\n",
            "iteration 600 / 1500: loss 5.859340\n",
            "iteration 700 / 1500: loss 3.540649\n",
            "iteration 800 / 1500: loss 2.647644\n",
            "iteration 900 / 1500: loss 2.342468\n",
            "iteration 1000 / 1500: loss 2.221387\n",
            "iteration 1100 / 1500: loss 2.264252\n",
            "iteration 1200 / 1500: loss 2.218076\n",
            "iteration 1300 / 1500: loss 2.233157\n",
            "iteration 1400 / 1500: loss 2.217954\n",
            "iteration 0 / 1500: loss 1525.419914\n",
            "iteration 100 / 1500: loss 559.630054\n",
            "iteration 200 / 1500: loss 206.122449\n",
            "iteration 300 / 1500: loss 76.803218\n",
            "iteration 400 / 1500: loss 29.512165\n",
            "iteration 500 / 1500: loss 12.124286\n",
            "iteration 600 / 1500: loss 5.867418\n",
            "iteration 700 / 1500: loss 3.501720\n",
            "iteration 800 / 1500: loss 2.616876\n",
            "iteration 900 / 1500: loss 2.357552\n",
            "iteration 1000 / 1500: loss 2.233474\n",
            "iteration 1100 / 1500: loss 2.200231\n",
            "iteration 1200 / 1500: loss 2.175072\n",
            "iteration 1300 / 1500: loss 2.191629\n",
            "iteration 1400 / 1500: loss 2.153551\n",
            "iteration 0 / 1500: loss 1518.655532\n",
            "iteration 100 / 1500: loss 556.674727\n",
            "iteration 200 / 1500: loss 205.025199\n",
            "iteration 300 / 1500: loss 76.394916\n",
            "iteration 400 / 1500: loss 29.375027\n",
            "iteration 500 / 1500: loss 12.115271\n",
            "iteration 600 / 1500: loss 5.775441\n",
            "iteration 700 / 1500: loss 3.479268\n",
            "iteration 800 / 1500: loss 2.692747\n",
            "iteration 900 / 1500: loss 2.357532\n",
            "iteration 1000 / 1500: loss 2.213873\n",
            "iteration 1100 / 1500: loss 2.265486\n",
            "iteration 1200 / 1500: loss 2.184681\n",
            "iteration 1300 / 1500: loss 2.137465\n",
            "iteration 1400 / 1500: loss 2.205400\n",
            "iteration 0 / 1500: loss 1555.045868\n",
            "iteration 100 / 1500: loss 569.880968\n",
            "iteration 200 / 1500: loss 210.046081\n",
            "iteration 300 / 1500: loss 78.290187\n",
            "iteration 400 / 1500: loss 30.025793\n",
            "iteration 500 / 1500: loss 12.428259\n",
            "iteration 600 / 1500: loss 5.898522\n",
            "iteration 700 / 1500: loss 3.536666\n",
            "iteration 800 / 1500: loss 2.654515\n",
            "iteration 900 / 1500: loss 2.344903\n",
            "iteration 1000 / 1500: loss 2.273054\n",
            "iteration 1100 / 1500: loss 2.218348\n",
            "iteration 1200 / 1500: loss 2.179642\n",
            "iteration 1300 / 1500: loss 2.164136\n",
            "iteration 1400 / 1500: loss 2.180863\n",
            "iteration 0 / 1500: loss 1561.106038\n",
            "iteration 100 / 1500: loss 572.179358\n",
            "iteration 200 / 1500: loss 210.878018\n",
            "iteration 300 / 1500: loss 78.574357\n",
            "iteration 400 / 1500: loss 30.154852\n",
            "iteration 500 / 1500: loss 12.428095\n",
            "iteration 600 / 1500: loss 5.929562\n",
            "iteration 700 / 1500: loss 3.518456\n",
            "iteration 800 / 1500: loss 2.661748\n",
            "iteration 900 / 1500: loss 2.391481\n",
            "iteration 1000 / 1500: loss 2.245757\n",
            "iteration 1100 / 1500: loss 2.194174\n",
            "iteration 1200 / 1500: loss 2.188584\n",
            "iteration 1300 / 1500: loss 2.149614\n",
            "iteration 1400 / 1500: loss 2.206980\n",
            "iteration 0 / 1500: loss 1540.368162\n",
            "iteration 100 / 1500: loss 564.045035\n",
            "iteration 200 / 1500: loss 207.854042\n",
            "iteration 300 / 1500: loss 77.442097\n",
            "iteration 400 / 1500: loss 29.747503\n",
            "iteration 500 / 1500: loss 12.262326\n",
            "iteration 600 / 1500: loss 5.998918\n",
            "iteration 700 / 1500: loss 3.485807\n",
            "iteration 800 / 1500: loss 2.656794\n",
            "iteration 900 / 1500: loss 2.351412\n",
            "iteration 1000 / 1500: loss 2.229205\n",
            "iteration 1100 / 1500: loss 2.202549\n",
            "iteration 1200 / 1500: loss 2.235771\n",
            "iteration 1300 / 1500: loss 2.189266\n",
            "iteration 1400 / 1500: loss 2.206932\n",
            "iteration 0 / 1500: loss 1548.886977\n",
            "iteration 100 / 1500: loss 567.394518\n",
            "iteration 200 / 1500: loss 208.952372\n",
            "iteration 300 / 1500: loss 77.866812\n",
            "iteration 400 / 1500: loss 29.923340\n",
            "iteration 500 / 1500: loss 12.363014\n",
            "iteration 600 / 1500: loss 5.874849\n",
            "iteration 700 / 1500: loss 3.562411\n",
            "iteration 800 / 1500: loss 2.637635\n",
            "iteration 900 / 1500: loss 2.298335\n",
            "iteration 1000 / 1500: loss 2.318359\n",
            "iteration 1100 / 1500: loss 2.238457\n",
            "iteration 1200 / 1500: loss 2.141035\n",
            "iteration 1300 / 1500: loss 2.173555\n",
            "iteration 1400 / 1500: loss 2.171712\n",
            "iteration 0 / 1500: loss 1537.064635\n",
            "iteration 100 / 1500: loss 563.255570\n",
            "iteration 200 / 1500: loss 207.395213\n",
            "iteration 300 / 1500: loss 77.282762\n",
            "iteration 400 / 1500: loss 29.699940\n",
            "iteration 500 / 1500: loss 12.226356\n",
            "iteration 600 / 1500: loss 5.857523\n",
            "iteration 700 / 1500: loss 3.581243\n",
            "iteration 800 / 1500: loss 2.653125\n",
            "iteration 900 / 1500: loss 2.406755\n",
            "iteration 1000 / 1500: loss 2.244653\n",
            "iteration 1100 / 1500: loss 2.144435\n",
            "iteration 1200 / 1500: loss 2.168226\n",
            "iteration 1300 / 1500: loss 2.129832\n",
            "iteration 1400 / 1500: loss 2.152470\n",
            "iteration 0 / 1500: loss 1557.812712\n",
            "iteration 100 / 1500: loss 570.678121\n",
            "iteration 200 / 1500: loss 210.264098\n",
            "iteration 300 / 1500: loss 78.337852\n",
            "iteration 400 / 1500: loss 30.116426\n",
            "iteration 500 / 1500: loss 12.380776\n",
            "iteration 600 / 1500: loss 5.886207\n",
            "iteration 700 / 1500: loss 3.549003\n",
            "iteration 800 / 1500: loss 2.682070\n",
            "iteration 900 / 1500: loss 2.323378\n",
            "iteration 1000 / 1500: loss 2.304784\n",
            "iteration 1100 / 1500: loss 2.162090\n",
            "iteration 1200 / 1500: loss 2.094521\n",
            "iteration 1300 / 1500: loss 2.215043\n",
            "iteration 1400 / 1500: loss 2.150070\n",
            "iteration 0 / 1500: loss 1548.358567\n",
            "iteration 100 / 1500: loss 567.425764\n",
            "iteration 200 / 1500: loss 208.963215\n",
            "iteration 300 / 1500: loss 77.807470\n",
            "iteration 400 / 1500: loss 29.923475\n",
            "iteration 500 / 1500: loss 12.355851\n",
            "iteration 600 / 1500: loss 5.888974\n",
            "iteration 700 / 1500: loss 3.573141\n",
            "iteration 800 / 1500: loss 2.660649\n",
            "iteration 900 / 1500: loss 2.355693\n",
            "iteration 1000 / 1500: loss 2.213161\n",
            "iteration 1100 / 1500: loss 2.204659\n",
            "iteration 1200 / 1500: loss 2.118691\n",
            "iteration 1300 / 1500: loss 2.254765\n",
            "iteration 1400 / 1500: loss 2.211618\n",
            "iteration 0 / 1500: loss 1550.853744\n",
            "iteration 100 / 1500: loss 568.743553\n",
            "iteration 200 / 1500: loss 209.576161\n",
            "iteration 300 / 1500: loss 78.043314\n",
            "iteration 400 / 1500: loss 29.935649\n",
            "iteration 500 / 1500: loss 12.326529\n",
            "iteration 600 / 1500: loss 5.886660\n",
            "iteration 700 / 1500: loss 3.504598\n",
            "iteration 800 / 1500: loss 2.647478\n",
            "iteration 900 / 1500: loss 2.363831\n",
            "iteration 1000 / 1500: loss 2.255062\n",
            "iteration 1100 / 1500: loss 2.200826\n",
            "iteration 1200 / 1500: loss 2.176086\n",
            "iteration 1300 / 1500: loss 2.168608\n",
            "iteration 1400 / 1500: loss 2.159193\n",
            "iteration 0 / 1500: loss 1560.486812\n",
            "iteration 100 / 1500: loss 571.844044\n",
            "iteration 200 / 1500: loss 210.496311\n",
            "iteration 300 / 1500: loss 78.397509\n",
            "iteration 400 / 1500: loss 30.045833\n",
            "iteration 500 / 1500: loss 12.309060\n",
            "iteration 600 / 1500: loss 5.942606\n",
            "iteration 700 / 1500: loss 3.574594\n",
            "iteration 800 / 1500: loss 2.727086\n",
            "iteration 900 / 1500: loss 2.345573\n",
            "iteration 1000 / 1500: loss 2.231293\n",
            "iteration 1100 / 1500: loss 2.180074\n",
            "iteration 1200 / 1500: loss 2.196526\n",
            "iteration 1300 / 1500: loss 2.191580\n",
            "iteration 1400 / 1500: loss 2.194396\n",
            "iteration 0 / 1500: loss 1558.686288\n",
            "iteration 100 / 1500: loss 571.449716\n",
            "iteration 200 / 1500: loss 210.444371\n",
            "iteration 300 / 1500: loss 78.427839\n",
            "iteration 400 / 1500: loss 30.094569\n",
            "iteration 500 / 1500: loss 12.355376\n",
            "iteration 600 / 1500: loss 5.936915\n",
            "iteration 700 / 1500: loss 3.543335\n",
            "iteration 800 / 1500: loss 2.741118\n",
            "iteration 900 / 1500: loss 2.321813\n",
            "iteration 1000 / 1500: loss 2.266592\n",
            "iteration 1100 / 1500: loss 2.241546\n",
            "iteration 1200 / 1500: loss 2.206218\n",
            "iteration 1300 / 1500: loss 2.144313\n",
            "iteration 1400 / 1500: loss 2.121625\n",
            "iteration 0 / 1500: loss 1540.596681\n",
            "iteration 100 / 1500: loss 564.389778\n",
            "iteration 200 / 1500: loss 207.935637\n",
            "iteration 300 / 1500: loss 77.570174\n",
            "iteration 400 / 1500: loss 29.729350\n",
            "iteration 500 / 1500: loss 12.213039\n",
            "iteration 600 / 1500: loss 5.862445\n",
            "iteration 700 / 1500: loss 3.579457\n",
            "iteration 800 / 1500: loss 2.664618\n",
            "iteration 900 / 1500: loss 2.389557\n",
            "iteration 1000 / 1500: loss 2.248304\n",
            "iteration 1100 / 1500: loss 2.187063\n",
            "iteration 1200 / 1500: loss 2.136549\n",
            "iteration 1300 / 1500: loss 2.201432\n",
            "iteration 1400 / 1500: loss 2.175138\n",
            "iteration 0 / 1500: loss 1525.668388\n",
            "iteration 100 / 1500: loss 558.866999\n",
            "iteration 200 / 1500: loss 205.914397\n",
            "iteration 300 / 1500: loss 76.780128\n",
            "iteration 400 / 1500: loss 29.414763\n",
            "iteration 500 / 1500: loss 12.210634\n",
            "iteration 600 / 1500: loss 5.885132\n",
            "iteration 700 / 1500: loss 3.552233\n",
            "iteration 800 / 1500: loss 2.611687\n",
            "iteration 900 / 1500: loss 2.352620\n",
            "iteration 1000 / 1500: loss 2.221804\n",
            "iteration 1100 / 1500: loss 2.202599\n",
            "iteration 1200 / 1500: loss 2.183043\n",
            "iteration 1300 / 1500: loss 2.233808\n",
            "iteration 1400 / 1500: loss 2.109907\n",
            "iteration 0 / 1500: loss 1515.514990\n",
            "iteration 100 / 1500: loss 555.181543\n",
            "iteration 200 / 1500: loss 204.692889\n",
            "iteration 300 / 1500: loss 76.239803\n",
            "iteration 400 / 1500: loss 29.294204\n",
            "iteration 500 / 1500: loss 12.094587\n",
            "iteration 600 / 1500: loss 5.788728\n",
            "iteration 700 / 1500: loss 3.518941\n",
            "iteration 800 / 1500: loss 2.673655\n",
            "iteration 900 / 1500: loss 2.365366\n",
            "iteration 1000 / 1500: loss 2.285699\n",
            "iteration 1100 / 1500: loss 2.165134\n",
            "iteration 1200 / 1500: loss 2.153575\n",
            "iteration 1300 / 1500: loss 2.202186\n",
            "iteration 1400 / 1500: loss 2.174769\n",
            "iteration 0 / 1500: loss 1540.081360\n",
            "iteration 100 / 1500: loss 564.555321\n",
            "iteration 200 / 1500: loss 207.962317\n",
            "iteration 300 / 1500: loss 77.497099\n",
            "iteration 400 / 1500: loss 29.729388\n",
            "iteration 500 / 1500: loss 12.205433\n",
            "iteration 600 / 1500: loss 5.862557\n",
            "iteration 700 / 1500: loss 3.544566\n",
            "iteration 800 / 1500: loss 2.699667\n",
            "iteration 900 / 1500: loss 2.358909\n",
            "iteration 1000 / 1500: loss 2.300665\n",
            "iteration 1100 / 1500: loss 2.172926\n",
            "iteration 1200 / 1500: loss 2.143106\n",
            "iteration 1300 / 1500: loss 2.200281\n",
            "iteration 1400 / 1500: loss 2.169422\n",
            "iteration 0 / 1500: loss 1562.620860\n",
            "iteration 100 / 1500: loss 572.602802\n",
            "iteration 200 / 1500: loss 210.868048\n",
            "iteration 300 / 1500: loss 78.573080\n",
            "iteration 400 / 1500: loss 30.176045\n",
            "iteration 500 / 1500: loss 12.409974\n",
            "iteration 600 / 1500: loss 5.862755\n",
            "iteration 700 / 1500: loss 3.554343\n",
            "iteration 800 / 1500: loss 2.654097\n",
            "iteration 900 / 1500: loss 2.411713\n",
            "iteration 1000 / 1500: loss 2.178663\n",
            "iteration 1100 / 1500: loss 2.171523\n",
            "iteration 1200 / 1500: loss 2.184121\n",
            "iteration 1300 / 1500: loss 2.197117\n",
            "iteration 1400 / 1500: loss 2.191239\n",
            "iteration 0 / 1500: loss 1525.699362\n",
            "iteration 100 / 1500: loss 559.343100\n",
            "iteration 200 / 1500: loss 205.939414\n",
            "iteration 300 / 1500: loss 76.821866\n",
            "iteration 400 / 1500: loss 29.480174\n",
            "iteration 500 / 1500: loss 12.117330\n",
            "iteration 600 / 1500: loss 5.809100\n",
            "iteration 700 / 1500: loss 3.530368\n",
            "iteration 800 / 1500: loss 2.682191\n",
            "iteration 900 / 1500: loss 2.433398\n",
            "iteration 1000 / 1500: loss 2.222473\n",
            "iteration 1100 / 1500: loss 2.175573\n",
            "iteration 1200 / 1500: loss 2.200699\n",
            "iteration 1300 / 1500: loss 2.178789\n",
            "iteration 1400 / 1500: loss 2.224011\n",
            "lr 1.000000e-08 reg 1.000000e+03 train accuracy: 0.329429 val accuracy: 0.337000\n",
            "lr 1.000000e-08 reg 5.623413e+03 train accuracy: 0.327898 val accuracy: 0.341000\n",
            "lr 1.000000e-08 reg 3.162278e+04 train accuracy: 0.322408 val accuracy: 0.343000\n",
            "lr 1.000000e-08 reg 1.778279e+05 train accuracy: 0.331061 val accuracy: 0.344000\n",
            "lr 1.000000e-08 reg 1.000000e+06 train accuracy: 0.328571 val accuracy: 0.337000\n",
            "lr 5.623413e-08 reg 1.000000e+03 train accuracy: 0.326939 val accuracy: 0.344000\n",
            "lr 5.623413e-08 reg 5.623413e+03 train accuracy: 0.330224 val accuracy: 0.341000\n",
            "lr 5.623413e-08 reg 3.162278e+04 train accuracy: 0.328224 val accuracy: 0.349000\n",
            "lr 5.623413e-08 reg 1.778279e+05 train accuracy: 0.326204 val accuracy: 0.345000\n",
            "lr 5.623413e-08 reg 1.000000e+06 train accuracy: 0.326816 val accuracy: 0.341000\n",
            "lr 3.162278e-07 reg 1.000000e+03 train accuracy: 0.330286 val accuracy: 0.350000\n",
            "lr 3.162278e-07 reg 5.623413e+03 train accuracy: 0.330184 val accuracy: 0.344000\n",
            "lr 3.162278e-07 reg 3.162278e+04 train accuracy: 0.329980 val accuracy: 0.350000\n",
            "lr 3.162278e-07 reg 1.778279e+05 train accuracy: 0.330776 val accuracy: 0.342000\n",
            "lr 3.162278e-07 reg 1.000000e+06 train accuracy: 0.329551 val accuracy: 0.336000\n",
            "lr 1.778279e-06 reg 1.000000e+03 train accuracy: 0.323939 val accuracy: 0.331000\n",
            "lr 1.778279e-06 reg 5.623413e+03 train accuracy: 0.328204 val accuracy: 0.344000\n",
            "lr 1.778279e-06 reg 3.162278e+04 train accuracy: 0.332592 val accuracy: 0.342000\n",
            "lr 1.778279e-06 reg 1.778279e+05 train accuracy: 0.330286 val accuracy: 0.349000\n",
            "lr 1.778279e-06 reg 1.000000e+06 train accuracy: 0.324592 val accuracy: 0.332000\n",
            "lr 1.000000e-05 reg 1.000000e+03 train accuracy: 0.325469 val accuracy: 0.340000\n",
            "lr 1.000000e-05 reg 5.623413e+03 train accuracy: 0.327918 val accuracy: 0.341000\n",
            "lr 1.000000e-05 reg 3.162278e+04 train accuracy: 0.331469 val accuracy: 0.344000\n",
            "lr 1.000000e-05 reg 1.778279e+05 train accuracy: 0.327245 val accuracy: 0.346000\n",
            "lr 1.000000e-05 reg 1.000000e+06 train accuracy: 0.329204 val accuracy: 0.346000\n",
            "best validation accuracy achieved during cross-validation: 0.350000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Tyo11a1WJqiF",
        "colab_type": "code",
        "outputId": "c5c49079-b29a-440f-d86c-96b88a854c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# evaluate on test set\n",
        "# Evaluate the best softmax on test set\n",
        "y_test_pred = best_softmax.predict(X_test)\n",
        "test_accuracy = np.mean(y_test == y_test_pred)\n",
        "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax on raw pixels final test set accuracy: 0.342000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_m2zUezMJqiY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Inline Question** - *True or False*\n",
        "\n",
        "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
        "\n",
        "*Your answer*: Yes\n",
        "\n",
        "*Your explanation*: Because the hinge loss can be zero but not the cross-entropy loss (e^x is never 0)"
      ]
    },
    {
      "metadata": {
        "id": "G0q-oidFJqic",
        "colab_type": "code",
        "outputId": "51bc628d-578e-4c50-ba7a-029ed99bcd4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        }
      },
      "cell_type": "code",
      "source": [
        "# Visualize the learned weights for each class\n",
        "w = best_softmax.W[:-1,:] # strip out the bias\n",
        "w = w.reshape(32, 32, 3, 10)\n",
        "\n",
        "w_min, w_max = np.min(w), np.max(w)\n",
        "\n",
        "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    \n",
        "    # Rescale the weights to be between 0 and 255\n",
        "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "    plt.imshow(wimg.astype('uint8'))\n",
        "    plt.axis('off')\n",
        "    plt.title(classes[i])"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAF7CAYAAAAkBgR2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x576 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Q6rixlVdvisb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}